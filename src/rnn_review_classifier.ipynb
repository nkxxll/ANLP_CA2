{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"scripts\")\n",
    "from scripts.review_dataloader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA version 12.1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Using CUDA version {torch.version.cuda}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Using MPS backend\")\n",
    "else:\n",
    "    print(\"No backend detected, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 10000 reviews\n"
     ]
    }
   ],
   "source": [
    "# review_dl = SteamReviewDataset(\"../data/reviews_100k.csv.bz2\", shuffle=True)\n",
    "\n",
    "reviews_df = pd.read_csv(\"../data/reviews_100k.csv.bz2\", low_memory=False)\n",
    "reviews_df[\"review\"] = reviews_df[\"review\"].astype(str)\n",
    "reviews_df \n",
    "\n",
    "# optionally shuffle (games are in order!)\n",
    "reviews_df = reviews_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# fewer data for testing\n",
    "# 100k full corpus takes 5h+\n",
    "# 10k takes 3 min\n",
    "# 1k takes 1s\n",
    "reviews_df = reviews_df[:10_000] \n",
    "\n",
    "print(f\"loaded {len(reviews_df)} reviews\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train true/false ratio is 0.69\n",
      "y_test true/false ratio is 0.69\n"
     ]
    }
   ],
   "source": [
    "# Daten aufsplitten\n",
    "x_train_raw, x_test_raw, y_train_raw, y_test_raw = train_test_split(reviews_df[\"review\"], reviews_df[\"voted_up\"], test_size=.33, random_state=42)   \n",
    "\n",
    "# Kontrolle\n",
    "print(f\"y_train true/false ratio is {len(y_train_raw[y_train_raw == True])/len(y_train_raw):.2f}\")\n",
    "print(f\"y_test true/false ratio is {len(y_test_raw[y_test_raw == True])/len(y_test_raw):.2f}\")      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab' 'ab open' 'ab open window' 'ab rb' 'ab rb or' 'abandon'\n",
      " 'abandon and' 'abandon and its' 'abandon and the' 'abandon any']\n"
     ]
    }
   ],
   "source": [
    "tfid_vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# ca. 45s\n",
    "x_train_tfidf = tfid_vectorizer.fit_transform(x_train_raw) # document term matrix\n",
    "x_test_tfidf = tfid_vectorizer.transform(x_test_raw)\n",
    "\n",
    "print(tfid_vectorizer.get_feature_names_out()[:10]) # Vorschau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.matrix'>\n",
      "(1, 561255)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How the data looks\n",
    "# can't load whole dataset als dense matrix (1.5 TB!)\n",
    "test: np.matrix = scipy.sparse.csr_matrix.todense(x_train_tfidf[2]) # from scipy.sparse.csr_matrix.todense(x_train)\n",
    "print(type(test))\n",
    "print(test.shape) # (1, 3317704)\n",
    "np.count_nonzero(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure PyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalDataLoader(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = np.array(y_train_raw)\n",
    "        print(self.reviews.shape)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.reviews.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = torch.tensor(scipy.sparse.csr_matrix.todense(self.reviews[idx]), dtype=torch.float32, device=torch.device('cuda:0')).squeeze(0)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32, device=torch.device('cuda:0'))\n",
    "        return review, label   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6700, 561255)\n",
      "6700\n"
     ]
    }
   ],
   "source": [
    "train_data_provider = MinimalDataLoader(x_train_tfidf, y_train_raw)\n",
    "print(len(train_data_provider))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'),\n",
       " tensor(0., device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_provider.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#     nn.Linear(train_data_provider.__getitem__(0)[0].shape[1], 1024),\n",
    "#     nn.Dropout(0.1),\n",
    "#     nn.Linear(1024, 512),\n",
    "#     nn.Dropout(0.1),                                     \n",
    "#     nn.Linear(512, 2),\n",
    "#     nn.Sigmoid() # 0 .. 1\n",
    "# )\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 1\n",
    "\n",
    "# data_it = iter(train_data_provider)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     data, label = next(data_it)\n",
    "#     data = torch.tensor(data).float()\n",
    "#     label = torch.tensor(label)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     output = model.forward(data)\n",
    "#     loss = criterion(output, label)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     # WIP\n",
    "\n",
    "def train_model(train_loader, input_dim, num_epochs=10, lr=0.001, device=\"cuda:0\"):\n",
    "    model = BinaryClassifier(input_dim=input_dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for batch_X, batch_y in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 210/210 [00:25<00:00,  8.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed. Average Loss: 0.4575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 210/210 [00:25<00:00,  8.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed. Average Loss: 0.0458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 210/210 [00:26<00:00,  7.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed. Average Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 210/210 [00:26<00:00,  7.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 completed. Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 210/210 [00:24<00:00,  8.58batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 completed. Average Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "# Assuming X_train, y_train are tf-idf and labels (from train-test split)\n",
    "batch_size = 32\n",
    "input_dim = x_train_tfidf.shape[1]\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "train_loader = DataLoader(train_data_provider, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(train_loader, input_dim=input_dim, num_epochs=5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
